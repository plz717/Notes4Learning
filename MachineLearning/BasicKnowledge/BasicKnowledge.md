#Loss Function
当损失函数是**凸函数**时, 可以保证我们找到的**局部最优即是全局最优**
- HInge Loss
> l(y)=max(0,1−t⋅y)
t与y均取值为±1， t为目标值， y为预测值;
当t为1时，若y小于1， 则l(y)>0, 函数会被往目标函数减小的方向优化， 即增大y的方向。
当y增大到1时， max(0, 1-ty) = 0;
但当y继续增大到大于1的时候，max(0, 1-ty)仍然为0, 并没有给出任何奖励，因此，并不鼓励分类器过于自信， 而有助于它更专注于样本整体的分类误差。

-  交叉熵损失
 > -ylog p(y|x)-(1-y)log (1-p(y|x))
 >本质上是让训练样本的分布与预测的分布尽量接近, 即p(y|x)与y尽量接近. 1-p(y|x)与1-y尽量接近. 


#正则化
- L1正则化
   > 倾向于使参数变为0，因此能产生稀疏解。实际应用时，数据的维度可能非常高，L1正则化使用的更为广泛一些。
-  L2正则化
    > 倾向于使参数大小相等.

#凸优化方法
-  梯度下降法
    -  **只用到了目标函数的一阶导数**信息（迭代方向）
    -   优点:计算简单;当损失函数是凸函数时, 可以找到全局最优
    -   缺点:在远离极小值的地方下降很快，而在靠近极小值的地方下降很慢。
-  牛顿法
    -  **用到了二阶导数信息**(因此会比一阶收敛更快)
    > 用目标函数的二阶泰勒展开近似该目标函数，通过求解这个二次函数的极小值来求解凸优化的搜索方向。
    -  优点:对于二次正定函数，迭代一次即可以得到最优解，对于非二次函数，若函数二次性较强或迭代点已经进入最优点的较小邻域，则收敛速度也很快。
    -   缺点:保证不了迭代方向是下降方向; 计算量相当复杂(计算梯度, 二阶偏导数矩阵和它的逆矩阵)
-  共轭梯度下降
-  LBFGS


