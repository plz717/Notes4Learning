# GBDT

GBDT主要由三个概念组成：

- 决策回归树
- 梯度提升
- Shrinkage (目前大部分源码都按该版本实现）

GBDT是一个加法模型，这个加法模型是由K个基模型组成的：

$f_{K}(x)=\sum_{k=1}^K T_{k}(x)$

我们希望这个组合后的模型$f_{K}(x)$可以**最小化**目标函数（损失函数）：

$L=\sum_{i=1}^n l(y_i, f_{K}(x_{i})) + \sum_{k=1}^K \Omega(T_k)$

第一项为经验风险，第二项为结构风险。

**那么如何得到这k个基模型呢？这是一个复杂的优化问题！！**

> 前向优化算法来啦！它的想法是，**从一个初始模型出发，逐渐改变模型来使得目标函数逐渐减小**，直至最优, 过程中的每次模型变化都用学习器来拟合，这些学习器就是基学习器， 将所有这些变化（基学习器）加在初始模型上，就得到了最终的最优模型。

那么如何改变基模型呢？

假设初始模型为$f_0(x) = T_{0}(x)$，那么为了最小化$L$,就要**让基模型沿着使得$L$下降的方向变化**：

$f_{1}(x) = f_{0}(x) - \frac{\partial L(y, f_{0}(x))}{\partial f_{0}(x)}$

因此我们只需要在当前基模型$f_{0}(x)$的基础上再加上一个变化$T_{1}(x)$,且满足：

$T_{1}(x) = - \frac{\partial L(y, f_{0}(x))}{\partial f_{0}(x)} （即负梯度）$

在梯度提升树中，用回归树来拟合这个变化$T(x)$。变化后的新模型$f_{1}(x) = f_{0}(x) + T_{1}(x)$就使得损失函数减小了一些啦！

这样继续下去，依次求出基模型每一次需要的变化$T_{i}(x)$， 并将所有这些变化加在初始的基模型上， 就可以得到使得损失函数最小的最终模型啦！

$f_0(x) = T_{0}(x)$

$f_1(x) = f_0(x) + T_{1}(x)$

$f_2(x) = f_1(x) + T_{2}(x)$

...

$f_K(x) = f_{K-1}(x) + T_{K}(x) = \sum_{k=1}^K T_k(x)$

###GBDT中的残差理解：

> 首先！残差只是针对采用平方损失函数的学习器来说的！在这种条件下：
>
> **每一棵树学的是之前所有树的预测值加和与真实值之间的残差**。比如A的真实年龄是18岁，但第一棵树的预测年龄是12岁，差了6岁。那么我们就让第二棵树去拟合6，如果第二棵树真的能把A分到6岁的叶子节点，那**累加两棵树**的结论就是A的真实年龄；如果第二棵树的结论是5岁，则A仍然存在1岁的残差，第三棵树里A的年龄就变成1岁，继续学。

### Shrinkage

> 每次走一小步逐渐逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易避免过拟合（没有理论证明，只有实验证明）。

不同于下式中，每次都让一个回归树去拟合损失函数在当前模型处的负梯度。

$f_{1}(x) = f_{0}(x) - \frac{\partial L(y, f_{0}(x))}{\partial f_{0}(x)}$

shrinkage认为模型的变化不能太快，也就是每次都让一个回归树拟合负梯度的一小部分。

$f_{1}(x) = f_{0}(x) -\eta \frac{\partial L(y, f_{0}(x))}{\partial f_{0}(x)}$

$f_{1}(x) = f_{0}(x) - \frac{\partial L(y, f_{0}(x))}{\partial f_{0}(x)}$

shrinkage认为每一次模型的变化都要尽量小，也就是以一个较小的速度变化



### GBDT与Adaboost的关系

-  AdaBoost

  - 采用**指数损失**函数、实现**二分类**学习任务的**前向加法**模型
  - 对异常点较敏感（因为异常点会有较大的误差，在前向学习的过程中会占有较大的样本权重）

- GBDT：

  - 采用**一般损失**函数（可微分就行）、可实现**多分类或回归**任务的**前向加法**模型

  - 可通过以下手段来降低基学习器（单颗决策树）的复杂度，从而减轻过拟合：

     -   可引入bagging中的bootstrap（只使用部分样本训练基学习器，即样本扰动）
     -   只随机采样部分属性（属性扰动）
     -   在目标函数中加入正则项
     -   限制单颗树的最大深度
     -   限制单颗树的叶子节点数目
     -   限制节点分裂时的最少样本数量

    ​





