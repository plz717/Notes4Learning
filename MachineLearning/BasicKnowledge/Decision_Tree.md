#Decision Tree
**三要素**:特征选择/决策树的生成/剪枝
**常用算法**:ID3/C4.5/CART
**结构**:
	多叉树:内部结点表示样本属性(即特征), 叶子结点表示类别
	互斥且完备:每一个实例都有且只有一条路径
**本质**:决策树本质是一个定义在特征空间划分上的条件概率分布, 在每个特征空间上, 都**强制**将此特征空间上的样本分配到**条件概率大的那一类**上.
> 类似KD树上的空间划分

**算法的学习过程**:
	假设特征集合为A={a1, a2, ..., ai, ..., an}
	针对当前的数据集D, 从当前特征集合中**选择最优特征**ai, 作为当前结点, 根据此特征对当前数据集分割成多个split, 针对每个split_i, 其数据集合变为Di, 特征集合为A-ai, 从此特征集合中再次选择最优特征, 并对其数据集合Di进行分割, 不断重复...便生成了一个决策树.
**算法停止准则**:
-   特征集合为空
-   当前叶子结点上所有样本都被分类正确
-   某一类别的样本数目少于一个阈值

##特征选择
在于选择**对训练数据**具有分类能力的特征.
熵:度量随机变量的不确定性. 熵越大, 不确定性越大. 
$$H(X)=-\sum_{x} p(x)*log({p(x))}$$
>
条件熵：H(Y|X)表示在已知随机变量X的条件下，随机变量Y的不确定性
$$H(Y|X)=\sum_{x}p(x)H(Y|X=x)$$
这个公式贯穿整个决策树算法!!!

###1.信息增益(ID3算法)
得知特征A的信息后而使得数据集D的类别不确定性(即类别的熵)减少的程度, 即
$$g(D,A) = H(D) - H(D|A)$$
其中:
给定特征A, 数据集D的类别不确定性为:
$$H(D|A) = \sum_{i}A_{i}H(Di)$$
单纯利用信息增益进行特征选择, 等价于**极大似然估计**

###2.信息增益比(C4.5算法)
当某个特征的值较多时(比如学号这一特征, 会有很多个分支), 增加这一特征会使得数据集的类别不确定性减少很多(因为一个学号会唯一对应一个类别), 但是这样的树会很浅, 分支很多, 只能拟合训练数据, 无法泛化到测试实例, 因此需要对分支较多的特征进行惩罚
$$g_{R}(D,A) = \frac{g(D,A)}{H_{A}(D)}$$
其中, $$H_{A}(D)$$是特征A的取值的熵(假设特征A有三种取值a1,a2,a3, 则熵等于)
$$-p(a1)*log(p(a1))-p(a2)*log(p(a2))-p(a3)*log(p(a3))$$

###3.基尼指数(CART算法)
假设一个有k个类别的样本集合, 样本点属于第k类的概率是 $$p_{k}$$, 基尼指数是用来描述**连续两次放回抽样时, 抽中的样本类别不同的概率**
$$Gini(p) = 1-\sum_{k} p_{k}^2$$ 

在决策树中, 叶子结点存放了某一类别k的样本,其纯度越高, 对应的, 基尼指数会越小
集合D的基尼指数为:
$$Gini(D) = 1-\sum_{k} p_{k}^2$$ 
给定特征A, 集合D的基尼指数为:
$$Gini(D|A) = \sum_{i}A_{i}Gini(Di)$$
通俗的理解就是: 
>假设将集合D根据特征A(a1, a2)分为两个集合D1, D2, 那么集合D的基尼指数就是 
>                    p(a1)*D1的基尼指数 + p(a2)*D2的基尼指数

##剪枝
剪枝包括两个过程:

-   从生成树的地段不断剪枝, 直到根节点, 形成一个子树序列;
-   通过交叉验证(采用**验证集**), 根据**损失函数最小原则, 从中选择最优子树.

这里的损失函数包括两部分:
$$C_{\alpha}(T) = \sum_{t} N_{t}H_{t}(T) + \alpha |T|$$
其中, $$|T|$$为树的叶子结点个数, $$H_{t}(T)$$ 为第t个叶子结点的类别熵(也就是这个叶子结点上, 样本类别的不确定性), $$N_{t}$$ 为第t个叶子结点上的样本数目.
左边部分保证了模型对训练数据的**预测误差**, 右边表示**模型复杂度**, 整体就是一个**正则化的极大似然估计**
-   	左边部分可以理解为: 那个叶子结点的样本数目越多, 也就是这个叶子节点上数据越多, 其类别信息应该越准确, 即不确定性越小, 也就是 要优化使得$$N_{t}$$ 比较大的叶子结点上的$$H_{t}(T)$$ 较小; 叶子结点的数目越少, 相当于这个类别的样本数目不是很多, 那自然不确定性就比较大, 对其不确定性的优化力度就不需要那么狠啦.
-   	右边部分: $$\alpha$$越大, 会优化的使得叶子节点数目越少, 对应模型结构就越简单, 反之越复杂.
