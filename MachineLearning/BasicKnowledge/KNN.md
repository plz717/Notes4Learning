#KNN
给定一个测试实例, **在训练集**中找到与这个测试实例**距离最近**的**k**个实例, 这k个实例中的**大多数**属于某个类,则将这个测试实例归于这个类别.
输入:测试实例的特征向量
输出:测试实例的类别

三要素:k值的选择\距离度量\分类决策规则(如多数表决)

优点:
lazy-----只需数据集就可以对测试实例分类或回归, 不需要学习(训练)过程
缺点:
-   需要把所有样本放在内存, 内存要求高
-   计算量大, 每个测试实例分类的复杂度为O(n)
-   样本不平衡的话会对算法性能有严重影响, 特别是当某一类样本特别少的时候, 会更容易受噪声干扰
-   k参数对于算法的性能影响巨大,可通过**交叉验证**来找到较好的k值

##k值的选择
较小的k值, 只有与测试实例很接近的训练样本才会对分类有影响, 会使得模型变复杂, **过拟合**
k=1时就是最近邻算法
较大的k能避免噪声数据的影响, 但k过大也会使得分类边界模糊, 模型变简单, 容易**欠拟合**

##距离度量
闵式距离
马氏距离
Lp距离(p=1时曼哈顿距离; p=2时欧氏距离)
$$L_p(x_i, x_j) = \left( \sum_{l=1}^{n}| x_i^{(l)} - x_j^{(l)} |^p \right)^{\frac{1}{p}}$$

##分类决策规则
一般采用多数表决, 它等价于经验风险最小化(误分类概率最小化)

##维度灾难
距离计算一般考虑样本的所有维度. **与样本类别无关的属性越多, 维度灾难发生的可能越大**

> 和距离,相似性有关的算法都有可能发生维度灾难



##拓展
-  KNN可以用于分类或回归
-  Weighted kNN:根据不同样本与测试实例的距离, 分配不同的权重
	-  基于实例的权重KNN(将权重加到实例上)
	-  基于实例属性的权重KNN(将权重加到实例属性上)

-  反向消除(Backward Elimination)
	反复对数据集操作，如果删除某个样本属性后，精确度降低，就保留该属性。


##疑问
"较小的k值会获得较小的近似误差, 较大的估计误差中"  近似误差和估计误差应该如何理解???

##实用KNN的关键点
-  数据清理,包括尺度归一化(与距离相关的都要考虑), 外点检测(减小噪声影响)等
-  数据特征选择(尽量去除无关属性)
-  搜索结构的优化(kd-tree等)

###如何对训练数据进行**快速k近邻搜索**??
线性扫描:计算测试实例与每一个训练样本的距离-->在样本集很大的时候非常不现实
	kd-树是一种便于对k维空间中的数据进行快速检索的数据结构
	如果实例点随机分布, 则搜索的平均复杂度是O(logN);
	适用于**训练实例数目远远大于特征维度**; 当两者近似时, 速度下降到几乎线性扫描
	如果特征的维度是 D，样本的数量是 N, 则:
	-   kd 树算法的复杂度是 O(DlogN)
	-   穷算O(DN)


##kd-tree
每次选择一个维度Di来对K维数据进行划分，相当于用一个垂直于该维度Di的超平面将K维数据空间一分为二，平面一边的所有K维数据 在Di维度上的值小于平面另一边的所有K维数据对应维度上的值。如果我 们继续分别对这两个子K维空间进行如上的划分，又会得到新的子空间，对新的子空间又继续划分，重复以上过程直到每个子空间都不能再划分为止。以上就是构造 Kd-Tree的过程.

两个重要的问题：
1）每次对子空间的划分时，怎样确定在哪个维度上进行划分；
2）在某个维度上进行划分时，怎样确 保在这一维度上的划分得到的两个子集合的数量尽量相等，即左子树和右子树中的结点个数尽量相等。(以使二叉树尽量平衡)

###每次对子空间的划分时，怎样确定在哪个维度上进行划分
方案一:
轮着来，即如果这次选择了在第i维上进行数据划分，那下一次就在第j(j≠i)维上进行划分，例如：j = (i mod k) + 1。就像切豆腐时，先是竖着切一刀，切成两半后，再横着来一刀，就得到了很小的方块豆腐。

但是这种方法只适用于k维数据**在各个维度上分布的比较分散**的情况;

方案二:
最大方差法，即每次我们选择维度进行划分时，都选择具有最大方差维度。
它适用于数据在**某些维度上分布的特别集中**, 另一些维度上分布十分分散的情况, 想象要切的是一根木条.

###怎样确保在这一维度上的划分得到的两个子集合的数量尽量相等
将数据按这一维度上的值进行排序, 选择中位数作为划分结点

###Kd-Tree与一维二叉查找树之间的区别

二叉查找树：数据存放在树中的每个结点（根结点、中间结点、叶子结点）中；

Kd-Tree：**数据只存放在叶子结点**，而根结点和中间结点存放一些空间划分信息（例如划分维度、划分值）；

###构建好一棵Kd-Tree后，如何利用Kd-Tree进行最近邻查找

（1）将查询数据Q从根结点开始，按照Q与各个结点的比较结果向下访问Kd-Tree，直至达到叶子结点。

（2）进行回溯（Backtracking）操作，该操作是为了找到离Q更近的“最近邻点”。即判断未被访问过的分支里是否还有离Q更近的点。如果Q与其父结点下的未被访问过的分支之间的距离小于Dcur，则认为该分支中存在离P更近的数据，进入该结点，进行（1）步骤一样的查找过程，如果找到更近的数据点，则更新为当前的“最近邻点”Pcur，并更新Dcur。

###为什么kd-tree到了高维空间后查找效率就会下降
为了能够找到查询点Q在数据集合中的最近邻点，有一个重要的操作步骤：回溯, 需要在未被访问过的且与Q的超球面相交的区域中查找可能存在的最近邻点。**随着维度K的增大**，与Q的超球面相交的区域就会增多，这就意味着**需要回溯判断的树分支就会更多**，从而算法的查找效率便会下降很大。

如何减少回溯次数呢?
(1)根据实验效果, 设置最大回溯次数
(2)为了保证在最大回溯次数内找到的最近邻比较接近真实最近邻, 需要对回溯的树分支进行优先性排序. 因为**离Q更近的树分支存在Q的最近邻的可能性更高。**

这种算法叫做BBF(Best Bin First), 首次提出于SIFT论文
