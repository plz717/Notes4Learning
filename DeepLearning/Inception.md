#Inception-v1

Motivation and features：

-  5×5 卷积核的输出中的信息就和 3×3 卷积核的输出不同，又不同于最大池化核的输出……在任意给定层，我们怎么知道什么样的变换能提供最「有用」的信息呢？

  - 为什么不让模型选择？

    > Inception 模块会并行堆叠多个不同大小的滤波器(1x1, 3x3, 5x5)和一个3x3的pooling，并将它们的结果都连接到单一一个输出，**意味着不同尺度特征的融合**。但是这样随之带来的是计算成本的增长， 怎么办？

- 使用 1×1 卷积来执行降维！——减少通道数目

  	> 比如，使用 20 个 1×1 过滤器，一个大小为 64×64×100（具有 100 个特征映射）的输入可以被压缩到 64×64×20。
  	>
  	> 例如：上一层的输出为100x100x128，经过具有256个输出的5x5卷积层之后(stride=1，pad=2)，输出数据为100x100x256。其中，卷积层的参数为128x5x5x256。假如上一层输出先经过具有32个输出的1x1卷积层，再经过具有256个输出的5x5卷积层，那么最终的输出数据仍为为100x100x256，但卷积参数量已经减少为128x1x1x32 + 32x5x5x256，大约减少了4倍。

- 底层多用1x1卷积，高层多用3x3和5x5卷积。

  	> 这是因为越到高层，神经元对应的图像视野越大，太小的卷积核难以提取到有效特征，需要使用大一点的卷积核

  -  使用辅助分类节点（auxiliary classifiers），将多个层的feature map都用以分类， 相当于做了模型融合，避免过拟合

  #Inception-v2

  -  用两个3x3的卷积核代替一个5x5卷积  (保持感受野范围的同时又减少了参数量)
  - 增加batch normalization，使得学习率可以增大，训练变快；防止梯度消失或者爆炸

  # Inception-v3

- 将一个较大的二维卷积变成两个较小的一维卷积， 减小了参数量，减轻过拟合。  7x7变成1x7和7x1

  > **在网络的低层使用这种分解效果并不好，在中等大小(12x12~20x20)的feature map上使用效果才会更好**

  # Inception-v4

  -  结合了resnet，得到inception-resnet，收敛更快，精度更高。

  # **Xception**

  它的假设是：「跨通道的相关性和空间相关性是完全可分离的，最好不要联合映射它们。」

  首先求跨图像区域的相关性，然后再求跨通道的相关性。

  > Inception V3是先做1x1的卷积，再做3x3的卷积，这样就先将通道进行了合并，即通道卷积，然后再进行空间卷积，而Xception则正好相反，先进行空间的3x3卷积，再进行通道的1x1卷积。

  ​