

##相机模型
1.小孔成像模型
2.齐次坐标
3.相机坐标系到图像坐标系
4.图像坐标系到像素坐标系
5.世界坐标系到相机坐标系
6.像素坐标系到世界坐标系

##Kinect
**1.彩色图**
> 彩色图像含有R,G,B三个通道，每个通道占8个bit（也就是unsigned char），故称为8UC3（8位unsigend char, 3通道）结构。

**2.深度图**
> 深度图是单通道的图像，每个像素由16个bit组成（也就是C++里的unsigned short），**像素的值代表该点离传感器的距离**。通常1000的值代表1米，所以我们把camera_factor设置成1000. 这样，深度图里每个像素点的读数除以1000，就是它离你的真实距离了。

##SLAM与图像配准
SLAM是由“定位”（Localization)和“建图”（Mapping)两部分构成的。现在来看定位问题。要求解机器人的运动，首先要解决这样一个问题：**给定了两个图像，如何知道图像的运动关系呢？**
目前有基于特征的方法（feature-based）或直接的方法（direct method）来解，主流方法为基于特征的方法，包括ICP、PnP等。
###OpenCV中的图像配准
**1.计算关键点**
关键点是一种cv::KeyPoint的类型。KeyPoint结构中带有 Point2f pt 这个成员变量，指这个关键点的像素坐标。此外，有的关键点还有半径、角度等参数。

> vector< cv::KeyPoint > kp1, kp2; //定义了两个容器kp1和kp2，分别存放第一张图像和第二张图像的keypoints， <cv::KeyPoint>代表容器中存放数据的类型
_detector->detect( rgb1, kp1 );  //提取第一张图像关键点
_detector->detect( rgb2, kp2 ); //提取第二张图像关键点

**2.针对这些关键点周围的像素，计算其“描述子”**
描述子是一个cv::Mat的矩阵结构，它的每一行代表一个对应于Keypoint的特征向量。当两个keypoint的描述子越相似，说明这两个关键点也就越相似。
> cv::Mat desp1, desp2;   //desp1为第一张图像的描述子矩阵，每一行代表一个keypoint的特征向量
_descriptor->compute( rgb1, kp1, desp1 );
_descriptor->compute( rgb2, kp2, desp2 );

**3.匹配描述子**
> vector< cv::DMatch > matches; 
cv::FlannBasedMatcher matcher;
matcher.match( desp1, desp2, matches);

**4.筛选匹配，把距离太大的去掉，通常去掉大于四倍最小距离的匹配**
**5.用PnP算法计算图像间的运动关系**
> cv::solvePnPRansac( pts_obj, pts_img, cameraMatrix, cv::Mat(), rvec, tvec, false, 100, 1.0, 100, inliers );//需要第一个帧的三维点和第二个帧的图像点

当当前帧与前一帧匹配失败时，需要丢弃当前帧，读取下一帧。如何检测匹配失败呢？
>1.去掉goodmatch太少的帧，最少的goodmatch定义为：
min_good_match=10
2.去掉solvePnPRASNAC里，inlier较少的帧，同理定义为：
min_inliers=5
3.去掉求出来的变换矩阵太大的情况。因为假设运动是连贯的，两帧之间不会隔的太远：
max_norm=0.3

##拼接点云
点云的拼接，实质上是对点云做变换的过程。这个变换往往是用一个4x4的变换矩阵(transform matrix)T来描述的
> PCL里提供了点云的变换函数，只要给定了变换矩阵，就能得到输入点云经过变换后的输出点云：
pcl::transformPointCloud将旧点云变换成与新点云相同的视角，然后将加在新点云上，即可得到最终的点云。( input_cloud, output_cloud, T );
对已有的旧点云和新点云，首先根据transformPointCloud将旧点云变换成与新点云相同的视角，然后将加在新点云上，即可得到最终的点云。

**至此， 已经实现了一个视觉里程计(Visual Odometry)啦！只要不断地把进来的数据与上一帧对比，就可以得到完整的运动轨迹以及地图了！**
使用两两匹配搭建的视觉里程计有什么不足呢？
 > 一旦出现了错误匹配，整个程序就会跑飞。
误差会累积。常见的现象是：相机转过去的过程能够做对，但转回来之后则出现明显的偏差。需要通过**回环检测**来消除它。
效率方面不尽如人意。在线的点云显示比较费时。

##姿态图
由相机姿态构成的一个图（graph）。一个图由节点与边构成：在最简单的情况下，节点代表相机的各个姿态(四元数形式或矩阵形式），而边指的是两个节点间的变换矩阵。
$$\min E = \sum\limits_{i,j} \| x_i^* - T_{i,j} x_j^* \|_2^2 .$$
这里$$x_i^*$$表示$$x_i$$的估计值。
这是一个非线性平方优化问题，根据迭代策略的不同，又可分为Gauss-Netwon(GN)下山法，Levenberg-Marquardt(LM)方法等等。这个问题也称为Bundle Adjustment(BA)，我们通常使用LM方法优化这个非线性平方误差函数。
为什么说slam里的BA问题稀疏呢？因为同样的场景很少出现在许多位置中。这导致上面的pose graph中，图G离全图很远，只有少部分的节点存在直接边的联系。这就是姿态图的稀疏性。这一稀疏性使得BA方法在slam中广泛应用起来。

1.初始化关键帧序列，并把第一帧装进去。
2.对于新来的一帧，首先计算其与最后一个关键帧之间的运动关系：
-    若运动太大/太小或正确匹配点对数太少，均丢弃；
-    否则，当前帧为关键帧，向地图中加入当前帧对应在图中的点和边（当前帧与前一个关键帧之间的边）；并进入闭环检测：
	**近距离回环**：将当前帧与末尾m个关键帧一一匹配，若匹配上了，则向图中加一条边（当前帧与这个关键帧之间的边）
	**随机回环**：将当前帧与随机n个关键帧一一匹配，若匹配上了，则向图中加一条边（当前帧与这个关键帧之间的边）

3.当不再来新的帧时，就进入全局优化。
4.优化完毕后，拼接点云地图：
-   每次从给g2o中取出一帧，得到优化后的位姿及点云，并转换到与旧点云相同的视角，再与旧的点云加在一起。

##未来工作
-   更好的数学模型（新的滤波器/图优化理论）； 
-   新的视觉特征/不使用特征的直接方法；
-   动态物体/人的处理；
-   地图描述/点云地图优化/语义地图
-   长时间/大规模/自动化slam
-   等等……

###资料
相机模型（内参数，外参数）:http://www.cnblogs.com/wangguchangqing/p/8126333.html
凸透镜成像及小孔成像:http://blog.csdn.net/lsh_2013/article/details/47615309