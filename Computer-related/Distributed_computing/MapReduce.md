#思想
把很多很多数据通过map来归类，输出key/value pair值的集合作为中间结果, 通过reducer来收集具有相同中间key值的value值, 把同一类的数据进行处理。
map和reduce函数是要执行的任务，由master分配任务给worker执行。
![结构][结构]
[结构]: https://pic2.zhimg.com/80/f7fbb747fc3e7a42112f1ccf82cfa1c7_hd.jpg
map-reduce之所以有效是基于两个哲学（好吧，这是我自己定义的）, （1）大而化小 和 （2） 异而化同。  这两个应对了大数据中的volume和variety挑战。 回到 map-reduce概念上， 假设我们手上有很多复杂数据, **map的工作就是切分数据，并给他们分类**，分类的方式就是输出key,value对，key就是对应“类别”了。 分类之后，reducer拿到的是所有数据中的同类数据，这样处理就很容易了。

#Spark做图像相似度检测
1.将所有图片以序列格式保存在HDFS中
2.用map函数, 对每个图片进行特征提取
3.对所有提取到的特征用spark MLLib的kmeans算法进行聚类, 得到k个视觉单词
4.在聚类模型训练好之后, 对每个特征, 都能得到其属于哪一类, (即属于哪个视觉单词)这样每张图片就对应一个k维的向量
5.可以利用这个k-维向量, 对每张图片进行相似度检测


